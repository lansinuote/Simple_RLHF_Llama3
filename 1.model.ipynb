{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fef1180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 125, 1024])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "#Norm层\n",
    "class LlamaRMSNorm(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.ones(1024))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #[4, 125, 1024] -> [4, 125, 1]\n",
    "        var = x.pow(2).mean(2, keepdim=True)\n",
    "\n",
    "        #差不多相当于x除以自身的绝对值的均值,相当于一种缩放\n",
    "        #计算结果的均值总是在-1到1之间\n",
    "        #[4, 125, 1024] * [4, 125, 1] -> [4, 125, 1024]\n",
    "        x = x * (var + 1e-5).rsqrt()\n",
    "\n",
    "        #[1024] * [4, 125, 1024] -> [4, 125, 1024]\n",
    "        return self.weight * x\n",
    "\n",
    "\n",
    "LlamaRMSNorm()(torch.randn(4, 125, 1024)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f49b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 16, 32]), torch.Size([1, 16, 32]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#计算结果是常量,有必要的话可以保存起来节省计算资源\n",
    "@torch.no_grad()\n",
    "def llama_rotary_embedding(lens):\n",
    "    #[0.0000, 0.0625, 0.1250, 0.1875, 0.2500, 0.3125, 0.3750, 0.4375, 0.5000, 0.5625, 0.6250, 0.6875, 0.7500, 0.8125, 0.8750, 0.9375]\n",
    "    inv_freq = torch.arange(0, 32, 2) / 32\n",
    "\n",
    "    #[1.0000e+00, 4.4037e-01, 1.9392e-01, 8.5397e-02, 3.7606e-02, 1.6560e-02, 7.2927e-03, 3.2114e-03, 1.4142e-03, 6.2277e-04, 2.7425e-04, 1.2077e-04, 5.3183e-05, 2.3420e-05, 1.0313e-05, 4.5417e-06]\n",
    "    inv_freq = 1.0 / (50_0000.0**inv_freq)\n",
    "\n",
    "    #[16] -> [1, 16, 1]\n",
    "    inv_freq = inv_freq.reshape(1, 16, 1)\n",
    "\n",
    "    #[1, 1, 16]\n",
    "    position_ids = torch.arange(lens).reshape(1, 1, -1).float()\n",
    "\n",
    "    #[1, 16, 1] * [1, 16, 1] -> [1, 16, 16]\n",
    "    freqs = inv_freq.matmul(position_ids).transpose(1, 2)\n",
    "\n",
    "    #[1, 16, 16+16] -> [1, 16, 32]\n",
    "    emb = torch.cat((freqs, freqs), 2)\n",
    "\n",
    "    return emb.cos(), emb.sin()\n",
    "\n",
    "\n",
    "cos, sin = llama_rotary_embedding(16)\n",
    "\n",
    "cos.shape, sin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d47c5377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 125, 1024])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#简单线性层\n",
    "class LlamaMLP(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gate_proj = torch.nn.Linear(1024, 14336, bias=False)\n",
    "        self.up_proj = torch.nn.Linear(1024, 14336, bias=False)\n",
    "        self.down_proj = torch.nn.Linear(14336, 1024, bias=False)\n",
    "        self.act_fn = torch.nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #[4, 125, 1024] -> [4, 125, 14336]\n",
    "        left = self.act_fn(self.gate_proj(x))\n",
    "\n",
    "        #[4, 125, 1024] -> [4, 125, 14336]\n",
    "        right = self.up_proj(x)\n",
    "\n",
    "        #[4, 125, 14336] -> [4, 125, 1024]\n",
    "        return self.down_proj(left * right)\n",
    "\n",
    "\n",
    "LlamaMLP()(torch.randn(4, 125, 1024)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea945117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 32, 125, 32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_rotary_pos_emb(x, cos, sin):\n",
    "    #x -> [4, 32, 125, 32]\n",
    "    #sin -> [1, 125, 32]\n",
    "    #cos -> [1, 125, 32]\n",
    "\n",
    "    def rotate_half(x):\n",
    "        #x -> [4, 32, 125, 32]\n",
    "\n",
    "        #[4, 32, 125, 32] -> [4, 32, 125, 16]\n",
    "        left = x[..., :16]\n",
    "        right = -x[..., 16:]\n",
    "\n",
    "        #[4, 32, 125, 16+16] -> [4, 32, 125, 32]\n",
    "        return torch.cat((right, left), -1)\n",
    "\n",
    "    #[1, 125, 32] -> [1, 1, 125, 32]\n",
    "    cos = cos.unsqueeze(1)\n",
    "    #[1, 125, 32] -> [1, 1, 125, 32]\n",
    "    sin = sin.unsqueeze(1)\n",
    "\n",
    "    #[4, 32, 125, 32] * [1, 1, 125, 32] + [4, 32, 125, 32] * [1, 1, 125, 32] -> [4, 32, 125, 32]\n",
    "    x = (x * cos) + (rotate_half(x) * sin)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "input = {\n",
    "    'x': torch.randn(4, 32, 125, 32),\n",
    "    'sin': torch.randn(1, 125, 32),\n",
    "    'cos': torch.randn(1, 125, 32)\n",
    "}\n",
    "apply_rotary_pos_emb(**input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e92aaeaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 125, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def repeat_kv(x):\n",
    "    shape = list(x.shape)\n",
    "    shape[1] *= 4\n",
    "    #[4, 2, 125, 32] -> [4, 2, 4, 125, 32] -> [4, 8, 125, 32]\n",
    "    return x.unsqueeze(2).repeat(1, 1, 4, 1, 1).reshape(shape)\n",
    "\n",
    "\n",
    "repeat_kv(torch.randn(4, 2, 125, 32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d74d732b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 125, 125])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#根据attention_mask获取注意力遮罩\n",
    "#遮罩值为0表示保留,min_value表示丢弃\n",
    "#遮罩的用法是和注意力矩阵相加后再求softmax\n",
    "def get_causal_mask(attention_mask):\n",
    "    # attention_mask -> [4, 125]\n",
    "\n",
    "    b, lens = attention_mask.shape\n",
    "    min_value = -1e15\n",
    "\n",
    "    #上三角矩阵,对角线以上为min_value,对角线以下为0,对角线为0\n",
    "    #[4, 1, 125, 125]\n",
    "    causal_mask = torch.full((lens, lens), min_value).triu(diagonal=1)\n",
    "    causal_mask = causal_mask.reshape(1, 1, lens, lens).repeat(b, 1, 1, 1)\n",
    "    causal_mask = causal_mask.to(attention_mask.device)\n",
    "\n",
    "    # 是pad的位置填充为min_value\n",
    "    # [4, 125] -> [4, 1, 1, 125]\n",
    "    mask = attention_mask.reshape(b, 1, 1, lens) == 0\n",
    "    # [4, 1, 125, 125]\n",
    "    causal_mask = causal_mask.masked_fill(mask, min_value)\n",
    "\n",
    "    return causal_mask\n",
    "\n",
    "\n",
    "get_causal_mask(torch.ones(4, 125).long()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac1c96aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 125, 1024])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#注意力层\n",
    "class LlamaAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.q_proj = torch.nn.Linear(1024, 1024, bias=False)\n",
    "        self.k_proj = torch.nn.Linear(1024, 256, bias=False)\n",
    "        self.v_proj = torch.nn.Linear(1024, 256, bias=False)\n",
    "        self.o_proj = torch.nn.Linear(1024, 1024, bias=False)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        # hidden_states -> [4, 125, 1024]\n",
    "        # attention_mask -> [4, 125]\n",
    "\n",
    "        b, lens, _ = hidden_states.shape\n",
    "\n",
    "        #线性投影,并拆分成多头注意力\n",
    "        # [4, 125, 1024] -> [4, 125, 1024] -> [4, 125, 32, 32] -> [4, 32, 125, 32]\n",
    "        q = self.q_proj(hidden_states).reshape(b, lens, 32, 32).transpose(1, 2)\n",
    "        # [4, 125, 1024] -> [4, 125, 256] -> [4, 125, 8, 32] -> [4, 8, 125, 32]\n",
    "        k = self.k_proj(hidden_states).reshape(b, lens, 8, 32).transpose(1, 2)\n",
    "        # [4, 125, 1024] -> [4, 125, 256] -> [4, 125, 8, 32] -> [4, 8, 125, 32]\n",
    "        v = self.v_proj(hidden_states).reshape(b, lens, 8, 32).transpose(1, 2)\n",
    "\n",
    "        #计算位置编码\n",
    "        # [1, 125, 32],[1, 125, 32]\n",
    "        cos, sin = llama_rotary_embedding(lens)\n",
    "        cos, sin = cos.to(hidden_states.device), sin.to(hidden_states.device)\n",
    "\n",
    "        #在q,k上应用位置编码\n",
    "        #[4, 32, 125, 32] -> [4, 32, 125, 32]\n",
    "        q = apply_rotary_pos_emb(q, cos, sin)\n",
    "        #[4, 8, 125, 32] -> [4, 8, 125, 32]\n",
    "        k = apply_rotary_pos_emb(k, cos, sin)\n",
    "\n",
    "        #k,v复制4分\n",
    "        # [4, 8, 125, 32] -> [4, 32, 125, 32]\n",
    "        k = repeat_kv(k)\n",
    "        # [4, 8, 125, 32] -> [4, 32, 125, 32]\n",
    "        v = repeat_kv(v)\n",
    "\n",
    "        #q,k,v连乘,计算注意力\n",
    "        # [4, 32, 125, 32] * [4, 32, 32, 125] -> [4, 32, 125, 125]\n",
    "        attn = q.matmul(k.transpose(2, 3)) / math.sqrt(32)\n",
    "\n",
    "        #根据attention_mask获得注意力遮罩\n",
    "        #[4, 125] -> [4, 1, 125, 125]\n",
    "        attention_mask = get_causal_mask(attention_mask)\n",
    "\n",
    "        #应用注意力遮罩\n",
    "        # [4, 32, 125, 125] + [4, 1, 125, 125] -> [4, 32, 125, 125]\n",
    "        attn = (attn + attention_mask).softmax(3)\n",
    "\n",
    "        #q,k,v连乘,计算注意力\n",
    "        # [4, 32, 125, 125] * [4, 32, 125, 32] -> [4, 32, 125, 32]\n",
    "        attn = attn.matmul(v)\n",
    "\n",
    "        #合并多头注意力\n",
    "        # [4, 32, 125, 32] -> [4, 125, 32, 32] -> [4, 125, 1024]\n",
    "        attn = attn.transpose(1, 2).reshape(b, lens, 1024)\n",
    "\n",
    "        #线性输出\n",
    "        # [4, 125, 1024] -> [4, 125, 1024]\n",
    "        attn = self.o_proj(attn)\n",
    "\n",
    "        return attn\n",
    "\n",
    "\n",
    "input = {\n",
    "    'hidden_states': torch.randn(4, 125, 1024),\n",
    "    'attention_mask': torch.ones(4, 125)\n",
    "}\n",
    "LlamaAttention()(**input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c0dbd7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 125, 1024])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LlamaDecoderLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaAttention()\n",
    "        self.mlp = LlamaMLP()\n",
    "        self.input_layernorm = LlamaRMSNorm()\n",
    "        self.post_attention_layernorm = LlamaRMSNorm()\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        #hidden_states -> [4, 125, 1024]\n",
    "        #attention_mask -> [4, 125]\n",
    "\n",
    "        res = hidden_states\n",
    "\n",
    "        #norm\n",
    "        #[4, 125, 1024] -> [4, 125, 1024]\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        #计算注意力,短接\n",
    "        #[4, 125, 1024],[4, 125] + [4, 125, 1024] -> [4, 125, 1024]\n",
    "        hidden_states = self.self_attn(hidden_states=hidden_states,\n",
    "                                       attention_mask=attention_mask) + res\n",
    "\n",
    "        res = hidden_states\n",
    "\n",
    "        #norm\n",
    "        #[4, 125, 1024] -> [4, 125, 1024]\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "\n",
    "        #线性计算,短接\n",
    "        #[4, 125, 1024] + [4, 125, 1024] -> [4, 125, 1024]\n",
    "        hidden_states = self.mlp(hidden_states) + res\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "input = {\n",
    "    'hidden_states': torch.randn(4, 125, 1024),\n",
    "    'attention_mask': torch.ones(4, 125).long()\n",
    "}\n",
    "LlamaDecoderLayer()(**input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80e06a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 125, 1024])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LlamaModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = torch.nn.Embedding(128256, 1024, None)\n",
    "        self.layers = torch.nn.ModuleList(\n",
    "            [LlamaDecoderLayer() for _ in range(4)])\n",
    "        self.norm = LlamaRMSNorm()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        #input_ids -> [4, 125]\n",
    "        #attention_mask -> [4, 125]\n",
    "\n",
    "        #编码\n",
    "        #[4, 125] -> [4, 125, 1024]\n",
    "        hidden_states = self.embed_tokens(input_ids)\n",
    "\n",
    "        #n层计算\n",
    "        for layer in self.layers:\n",
    "            #[4, 125, 1024] -> [4, 125, 1024]\n",
    "            hidden_states = layer(hidden_states, attention_mask=attention_mask)\n",
    "\n",
    "        #norm\n",
    "        #[4, 125, 1024] -> [4, 125, 1024]\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "input = {\n",
    "    'input_ids': torch.randint(100, 50000, [4, 125]),\n",
    "    'attention_mask': torch.ones(4, 125).long(),\n",
    "}\n",
    "\n",
    "input['attention_mask'][:, 120:] = 0\n",
    "\n",
    "LlamaModel()(**input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18641aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(11.9189, grad_fn=<NllLossBackward0>), torch.Size([4, 125, 128256]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LlamaForCausalLM(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel()\n",
    "        self.lm_head = torch.nn.Linear(1024, 128256, bias=False)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        #input_ids -> [4, 125]\n",
    "        #attention_mask -> [4, 125]\n",
    "        #labels -> [4, 125]\n",
    "\n",
    "        #[4, 125] -> [4, 125, 1024]\n",
    "        logits = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        #[4, 125, 1024] -> [4, 125, 128256]\n",
    "        logits = self.lm_head(logits)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[:, :-1].reshape(-1, 128256)\n",
    "            shift_labels = labels[:, 1:].reshape(-1)\n",
    "            loss = torch.nn.functional.cross_entropy(shift_logits,\n",
    "                                                     shift_labels)\n",
    "\n",
    "        return loss, logits\n",
    "\n",
    "\n",
    "input = {\n",
    "    'input_ids': torch.randint(100, 50000, [4, 125]),\n",
    "    'attention_mask': torch.ones(4, 125).long(),\n",
    "    'labels': torch.randint(100, 50000, [4, 125]),\n",
    "}\n",
    "\n",
    "input['attention_mask'][:, 120:] = 0\n",
    "\n",
    "loss, logits = LlamaForCausalLM()(**input)\n",
    "\n",
    "loss, logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "981f8f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.9679, grad_fn=<NllLossBackward0>) torch.Size([4, 125, 128256])\n",
      "tensor(11.9679, grad_fn=<NllLossBackward0>) torch.Size([4, 125, 128256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(True), tensor(True))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import LlamaConfig, LlamaForCausalLM as LlamaForCausalLM_Original\n",
    "\n",
    "# #测试是否和官方模型的计算输出一样\n",
    "# config = \"{'vocab_size': 128256, 'max_position_embeddings': 8192, 'hidden_size': 4096, 'intermediate_size': 14336, 'num_hidden_layers': 32, 'num_attention_heads': 32, 'num_key_value_heads': 8, 'hidden_act': 'silu', 'initializer_range': 0.02, 'rms_norm_eps': 1e-05, 'pretraining_tp': 1, 'use_cache': True, 'rope_theta': 500000.0, 'rope_scaling': None, 'attention_bias': False, 'attention_dropout': 0.0, 'mlp_bias': False, 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'bfloat16', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': False, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['LlamaForCausalLM'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': 128000, 'pad_token_id': None, 'eos_token_id': 128001, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': '', 'transformers_version': '4.38.2', 'model_type': 'llama'}\"\n",
    "# config = LlamaConfig.from_dict(eval(config))\n",
    "# config.hidden_size = 1024\n",
    "# config.num_hidden_layers = 4\n",
    "\n",
    "# model_actor1 = LlamaForCausalLM_Original(config)\n",
    "# model_actor2 = LlamaForCausalLM()\n",
    "\n",
    "# model_actor2.load_state_dict(model_actor1.state_dict())\n",
    "\n",
    "# input = {\n",
    "#     'input_ids': torch.randint(100, 50000, [4, 125]),\n",
    "#     'attention_mask': torch.ones(4, 125).long(),\n",
    "#     'labels': torch.randint(100, 50000, [4, 125])\n",
    "# }\n",
    "# input['attention_mask'][:, 120:] = 0\n",
    "\n",
    "# out = model_actor1(**input)\n",
    "# loss, logits = model_actor2(**input)\n",
    "\n",
    "# print(out.loss, out.logits.shape)\n",
    "# print(loss, logits.shape)\n",
    "\n",
    "# out.loss == loss, (out.logits == logits).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cuda117]",
   "language": "python",
   "name": "conda-env-cuda117-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
